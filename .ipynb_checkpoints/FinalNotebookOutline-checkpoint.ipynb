{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Sentiment Analysis for SXSW Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amelia Dahm and Eric Roberts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a id=\"Overview\"></a>\n",
    "\n",
    "\n",
    "-------------------- INSERT PARAGRAPH HERE --------------------\n",
    "\n",
    "[1. Business Problem](#Business-Problem)\n",
    "\n",
    "[2. Data Understanding and Preparation](#Data-Understanding)\n",
    "\n",
    "[3. EDA](#EDA)\n",
    "\n",
    "[4. Modeling](#Modeling)\n",
    "\n",
    "[5. Evaluation](#Evaluation)\n",
    "\n",
    "[6. Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem <a id=\"Business-Problem\"></a>\n",
    "##### [(back to top)](#Overview)\n",
    "\n",
    "-------------------- INSERT PARAGRAPH HERE --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding and Preperation <a id=\"Data-Understanding\"></a>\n",
    "##### [(back to top)](#Overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------- INSERT PARAGRAPH HERE --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the ones we dont use\n",
    "\n",
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.probability import FreqDist # looks at how frequent words are used\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from matplotlib import cm\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string, re\n",
    "\n",
    "\n",
    "# from neural net\n",
    "from __future__ import print_function\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>product_</th>\n",
       "      <th>emotion</th>\n",
       "      <th>lemmatizer_tweets</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <th>vs_neg</th>\n",
       "      <th>vs_neu</th>\n",
       "      <th>vs_pos</th>\n",
       "      <th>vs_compound</th>\n",
       "      <th>nrc_sentiment</th>\n",
       "      <th>gi_sentiment</th>\n",
       "      <th>henry_sentiment</th>\n",
       "      <th>huliu_sentiment</th>\n",
       "      <th>jockers_sentiment</th>\n",
       "      <th>lm_sentiment</th>\n",
       "      <th>senticnet_sentiment</th>\n",
       "      <th>sentiword_sentiment</th>\n",
       "      <th>socal_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['wesley83', 'have', '3G', 'iPhone', '3', 'hrs...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>wesley83 have 3G iPhone 3 hr tweeting RISE Aus...</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.09520</td>\n",
       "      <td>-0.221875</td>\n",
       "      <td>-1.192154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['jessedee', 'Know', 'fludapp', 'Awesome', 'iP...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>1</td>\n",
       "      <td>jessedee Know fludapp Awesome iPad iPhone app ...</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.47500</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>2.177190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['swonderlin', 'not', 'wait', 'iPad', '2', 'al...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>1</td>\n",
       "      <td>swonderlin not wait iPad 2 also should sale do...</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.625000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.30550</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['sxsw', 'hope', 'year', 'festival', 't', 'cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>0</td>\n",
       "      <td>sxsw hope year festival t crashy this year iPh...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07160</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.841547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['sxtxstate', 'great', 'stuff', 'Fri', 'SXSW',...</td>\n",
       "      <td>Google</td>\n",
       "      <td>1</td>\n",
       "      <td>sxtxstate great stuff Fri SXSW Marissa Mayer G...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.55125</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.554026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet            product_  \\\n",
       "0  ['wesley83', 'have', '3G', 'iPhone', '3', 'hrs...              iPhone   \n",
       "1  ['jessedee', 'Know', 'fludapp', 'Awesome', 'iP...  iPad or iPhone App   \n",
       "2  ['swonderlin', 'not', 'wait', 'iPad', '2', 'al...                iPad   \n",
       "3  ['sxsw', 'hope', 'year', 'festival', 't', 'cra...  iPad or iPhone App   \n",
       "4  ['sxtxstate', 'great', 'stuff', 'Fri', 'SXSW',...              Google   \n",
       "\n",
       "   emotion                                  lemmatizer_tweets  \\\n",
       "0        0  wesley83 have 3G iPhone 3 hr tweeting RISE Aus...   \n",
       "1        1  jessedee Know fludapp Awesome iPad iPhone app ...   \n",
       "2        1  swonderlin not wait iPad 2 also should sale do...   \n",
       "3        0  sxsw hope year festival t crashy this year iPh...   \n",
       "4        1  sxtxstate great stuff Fri SXSW Marissa Mayer G...   \n",
       "\n",
       "   textblob_polarity  textblob_subjectivity  vs_neg  vs_neu  vs_pos  \\\n",
       "0          -0.200000               0.400000   0.223   0.777   0.000   \n",
       "1           0.466667               0.933333   0.000   0.528   0.472   \n",
       "2          -0.155556               0.288889   0.000   1.000   0.000   \n",
       "3           0.000000               0.000000   0.000   0.596   0.404   \n",
       "4           0.800000               0.750000   0.000   0.796   0.204   \n",
       "\n",
       "   vs_compound  nrc_sentiment  gi_sentiment  henry_sentiment  huliu_sentiment  \\\n",
       "0      -0.6486            0.0     -0.333333              0.0             -1.0   \n",
       "1       0.9100            1.0      1.000000              0.0              1.0   \n",
       "2       0.0000           -1.0     -1.000000             -1.0             -1.0   \n",
       "3       0.7269            1.0      1.000000              0.0              0.0   \n",
       "4       0.6249            0.0      1.000000              0.0              1.0   \n",
       "\n",
       "   jockers_sentiment  lm_sentiment  senticnet_sentiment  sentiword_sentiment  \\\n",
       "0          -1.000000           0.0             -0.09520            -0.221875   \n",
       "1           0.416667           0.0              0.47500             0.175000   \n",
       "2          -0.625000          -1.0             -0.30550            -0.289062   \n",
       "3           0.500000           0.0              0.07160             0.250000   \n",
       "4           0.500000           1.0              0.55125             0.083333   \n",
       "\n",
       "   socal_sentiment  \n",
       "0        -1.192154  \n",
       "1         2.177190  \n",
       "2        -1.000000  \n",
       "3         2.841547  \n",
       "4         1.554026  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do we want to add cleaning steps here or link to noteboook with it aall\n",
    "\n",
    "#import dataset\n",
    "df = pd.read_csv('data/dataframe.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet                     object\n",
       "product_                  object\n",
       "emotion                    int64\n",
       "lemmatizer_tweets         object\n",
       "textblob_polarity        float64\n",
       "textblob_subjectivity    float64\n",
       "vs_neg                   float64\n",
       "vs_neu                   float64\n",
       "vs_pos                   float64\n",
       "vs_compound              float64\n",
       "nrc_sentiment            float64\n",
       "gi_sentiment             float64\n",
       "henry_sentiment          float64\n",
       "huliu_sentiment          float64\n",
       "jockers_sentiment        float64\n",
       "lm_sentiment             float64\n",
       "senticnet_sentiment      float64\n",
       "sentiword_sentiment      float64\n",
       "socal_sentiment          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA <a id=\"EDA\"></a>\n",
    "##### [(back to top)](#Overview)\n",
    "\n",
    "-------------------- INSERT PARAGRAPH HERE --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD CLOUDS woooo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling <a id=\"Modeling\"></a>\n",
    "##### [(back to top)](#Overview)\n",
    "\n",
    "-------------------- INSERT PARAGRAPH HERE --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  i think this can be deleted\n",
    "\n",
    "# data = df['lemmatizer_tweets']\n",
    "# target = df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list with all lemmatized outputs\n",
    "lemmatized_output = []\n",
    "\n",
    "for listy in data:\n",
    "    lemmed = ''.join([w for w in listy])\n",
    "    lemmatized_output.append(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting stopwords and punctuations\n",
    "stop_words=stopwords.words(\"english\")\n",
    "stop_words += list(string.punctuation)\n",
    "stop_words += ['...','u','w','2',\"i'm\",'via',\"we're\",'6','3','hey']\n",
    "# print(stop_words)\n",
    "sw_set = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lem = lemmatized_output\n",
    "y_lem = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split the lemmatized words\n",
    "X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X_lem, y_lem, test_size=0.20, random_state=1)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words=stop_words)\n",
    "\n",
    "tfidf_data_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "tfidf_data_test_lem = tfidf.transform(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.60415234428709\n",
      "0.9994728017458652\n"
     ]
    }
   ],
   "source": [
    "# Average Number of Non-Zero Elements in Vectorized Tweets\n",
    "non_zero_cols = tfidf_data_train_lem.nnz / float(tfidf_data_train_lem.shape[0])\n",
    "print(non_zero_cols)\n",
    "\n",
    "# Percentage of columns containing ZERO\n",
    "percent_sparse = 1 - (non_zero_cols / float(tfidf_data_train_lem.shape[1]))\n",
    "print(percent_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_lem = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1, random_state=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_lem.fit(tfidf_data_train_lem, y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_preds_lem = rf_lem.predict(tfidf_data_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6547553600879604\n",
      "Precision: 0.6493556607054446\n",
      "Recall: 0.6547553600879604\n",
      "F1: 0.6133479327245666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rf_acc_score_lem = metrics.accuracy_score(y_test_lem, rf_test_preds_lem)\n",
    "rf_f1_score_lem = metrics.f1_score(y_test_lem, rf_test_preds_lem, average='weighted')\n",
    "rf_precision_score_lem = metrics.precision_score(y_test_lem, rf_test_preds_lem, average='weighted')\n",
    "rf_recall_score_lem = metrics.recall_score(y_test_lem, rf_test_preds_lem, average='weighted')\n",
    "print('Accuracy:', rf_acc_score_lem)\n",
    "print('Precision:',rf_precision_score_lem)\n",
    "print('Recall:',rf_recall_score_lem)\n",
    "print('F1:',rf_f1_score_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [150, 200],\n",
    "    'max_features': ['auto'],\n",
    "    'max_depth' : [8],\n",
    "    'criterion' :['gini']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n",
       "             param_grid={'criterion': ['gini'], 'max_depth': [8],\n",
       "                         'max_features': ['auto'], 'n_estimators': [150, 200]})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(tfidf_data_train_lem, y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6016770893896208\n",
      "{'criterion': 'gini', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 150}\n",
      "RandomForestClassifier(max_depth=8, n_estimators=150, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "print(CV_rfc.best_score_)\n",
    "print(CV_rfc.best_params_)\n",
    "print(CV_rfc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotion'] = np.where(df['emotion'] == 4, 2, df['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns=['emotion', 'tweet', 'product_', 'lemmatizer_tweets'])\n",
    "target = df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.20, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7273, 15)\n",
      "(1819, 15)\n",
      "(7273,)\n",
      "(1819,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 22,204\n",
      "Trainable params: 22,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "910/910 [==============================] - 1s 865us/step - loss: 0.9447 - accuracy: 0.5933 - val_loss: 0.9115 - val_accuracy: 0.5816\n",
      "Epoch 2/15\n",
      "910/910 [==============================] - 1s 747us/step - loss: 0.8886 - accuracy: 0.6058 - val_loss: 0.9061 - val_accuracy: 0.5899\n",
      "Epoch 3/15\n",
      "910/910 [==============================] - 1s 721us/step - loss: 0.8773 - accuracy: 0.6070 - val_loss: 0.8999 - val_accuracy: 0.5937\n",
      "Epoch 4/15\n",
      "910/910 [==============================] - 1s 702us/step - loss: 0.8712 - accuracy: 0.6135 - val_loss: 0.8926 - val_accuracy: 0.6102\n",
      "Epoch 5/15\n",
      "910/910 [==============================] - 1s 731us/step - loss: 0.8684 - accuracy: 0.6114 - val_loss: 0.8933 - val_accuracy: 0.6102\n",
      "Epoch 6/15\n",
      "910/910 [==============================] - 1s 715us/step - loss: 0.8679 - accuracy: 0.6123 - val_loss: 0.8912 - val_accuracy: 0.6042\n",
      "Epoch 7/15\n",
      "910/910 [==============================] - 1s 735us/step - loss: 0.8598 - accuracy: 0.6157 - val_loss: 0.8894 - val_accuracy: 0.6009\n",
      "Epoch 8/15\n",
      "910/910 [==============================] - 1s 750us/step - loss: 0.8572 - accuracy: 0.6176 - val_loss: 0.8897 - val_accuracy: 0.6075\n",
      "Epoch 9/15\n",
      "910/910 [==============================] - 1s 698us/step - loss: 0.8568 - accuracy: 0.6186 - val_loss: 0.8897 - val_accuracy: 0.5954\n",
      "Epoch 10/15\n",
      "910/910 [==============================] - 1s 767us/step - loss: 0.8539 - accuracy: 0.6146 - val_loss: 0.8848 - val_accuracy: 0.6135\n",
      "Epoch 11/15\n",
      "910/910 [==============================] - 1s 713us/step - loss: 0.8537 - accuracy: 0.6202 - val_loss: 0.8898 - val_accuracy: 0.5992\n",
      "Epoch 12/15\n",
      "910/910 [==============================] - 1s 718us/step - loss: 0.8553 - accuracy: 0.6152 - val_loss: 0.8812 - val_accuracy: 0.6141\n",
      "Epoch 13/15\n",
      "910/910 [==============================] - 1s 730us/step - loss: 0.8500 - accuracy: 0.6165 - val_loss: 0.8832 - val_accuracy: 0.6146\n",
      "Epoch 14/15\n",
      "910/910 [==============================] - 1s 693us/step - loss: 0.8495 - accuracy: 0.6197 - val_loss: 0.8817 - val_accuracy: 0.6091\n",
      "Epoch 15/15\n",
      "910/910 [==============================] - 1s 683us/step - loss: 0.8522 - accuracy: 0.6186 - val_loss: 0.8791 - val_accuracy: 0.6113\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.20, random_state=1)\n",
    "\n",
    "# tl = TomekLinks()\n",
    "# X_train, y_train = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "batch_size = 8 # how many folds to separate data\n",
    "num_classes = 4 # how many classes in outcomes\n",
    "epochs = 15\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "y_train = y_train.astype('uint8')\n",
    "y_test = y_test.astype('uint8')\n",
    "\n",
    "\n",
    "# X_train = X_train.reshape(1738, 15)\n",
    "# X_test = X_test.reshape(1819, 15)\n",
    "# X_train = X_train.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# specifying the model structure\n",
    "model = Sequential()\n",
    "\n",
    "# specify the first hidden layer\n",
    "model.add(Dense(100, activation='relu', input_shape=(15,)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# specify the second layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# # specify the third layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# # specify the fourth layer\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# # specify the 5th layer\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# specify the output layer\n",
    "model.add(Dense(num_classes, activation='softmax')) # switched linear to sofmax\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8790899515151978\n",
      "Test accuracy: 0.6113249063491821\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation <a id=\"Evaluation\"></a>\n",
    "##### [(back to top)](#Overview)\n",
    "\n",
    "-------------------- INSERT PARAGRAPH HERE --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab our metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a id=\"Conclusion\"></a>\n",
    "##### [(back to top)](#Overview)\n",
    "\n",
    "-------------------- INSERT PARAGRAPH HERE --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
