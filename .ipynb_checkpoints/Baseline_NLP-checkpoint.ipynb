{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce project ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.probability import FreqDist # looks at how frequent words are used\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from matplotlib import cm\n",
    "from sklearn.ensemble import RandomForestClassifier #\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in tweets\n",
    "# src: https://data.world/crowdflower/brands-and-product-emotions\n",
    "df = pd.read_csv('data/tweets.csv', encoding = \"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df.rename(columns ={'tweet_text': 'tweet',\n",
    "                    'emotion_in_tweet_is_directed_at': 'product_',\n",
    "                    'is_there_an_emotion_directed_at_a_brand_or_product': 'emotion'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could make the nulls for products into 'Other' category\n",
    "\n",
    "# put null values into other category\n",
    "df['product_'] = np.where(df['product_'].isnull(), 'Unknown', df['product_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the one null tweet\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5388\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target variable\n",
    "# I CANT TELL whattt\n",
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign target varible numbers\n",
    "df['emotion'] = np.where(df['emotion'] == \"I can't tell\", 4, df['emotion'])\n",
    "df['emotion'] = np.where(df['emotion'] == 'No emotion toward brand or product', 3, df['emotion'])\n",
    "df['emotion'] = np.where(df['emotion'] == 'Positive emotion', 1, df['emotion'])\n",
    "df['emotion'] = np.where(df['emotion'] == 'Negative emotion', 0, df['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update data type\n",
    "df['emotion'] = df['emotion'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize words in tweets using regex\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "items = []\n",
    "for item in df['tweet']:\n",
    "    item = tokenizer.tokenize(item)\n",
    "    items.append(item)\n",
    "    \n",
    "df['tweet'] = items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stop wordset\n",
    "stop_words=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166994\n"
     ]
    }
   ],
   "source": [
    "# Before stop word removal\n",
    "before = 0\n",
    "for item in df['tweet']:\n",
    "    for obj in item:\n",
    "        before +=1\n",
    "print(before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to remove stop words\n",
    "removed_stop = []\n",
    "for row in df['tweet']:\n",
    "    for item in row:\n",
    "        if item.lower() in stop_words:\n",
    "            row.remove(item)\n",
    "    removed_stop.append(row)\n",
    "df['tweet'] = removed_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166994\n",
      "126936\n"
     ]
    }
   ],
   "source": [
    "# After stop word removal\n",
    "after = 0\n",
    "for item in df['tweet']:\n",
    "    for obj in item:\n",
    "        after +=1\n",
    "print(before)\n",
    "print(after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps = PorterStemmer()\n",
    "\n",
    "# stemmed_tweets=[]\n",
    "# for row in df['tweet']:\n",
    "#     new_row = []\n",
    "#     for w in row:\n",
    "#         new_row.append(ps.stem(w))\n",
    "#     stemmed_tweets.append(new_row)\n",
    "        \n",
    "# df['stemmed_tweets'] =  stemmed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "lemmatizer_tweets=[]\n",
    "for row in df['tweet']:\n",
    "    new_row = []\n",
    "    for w in row:\n",
    "        new_row.append(lemmatizer.lemmatize(w))\n",
    "    lemmatizer_tweets.append(new_row)\n",
    "        \n",
    "df['lemmatizer_tweets'] =  lemmatizer_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [wesley83, have, 3G, iPhone, 3, hr, tweeting, ...\n",
       "1       [jessedee, Know, fludapp, Awesome, iPad, iPhon...\n",
       "2       [swonderlin, not, wait, iPad, 2, also, should,...\n",
       "3       [sxsw, hope, year, festival, t, crashy, this, ...\n",
       "4       [sxtxstate, great, stuff, Fri, SXSW, Marissa, ...\n",
       "                              ...                        \n",
       "9088                       [Ipad, everywhere, SXSW, link]\n",
       "9089    [Wave, buzz, RT, mention, interrupt, regularly...\n",
       "9090    [Google, Zeiger, physician, never, reported, p...\n",
       "9091    [Verizon, iPhone, customer, complained, time, ...\n",
       "9092    [RT, mention, Google, Tests, Check, Offers, SX...\n",
       "Name: lemmatizer_tweets, Length: 9092, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemmatizer_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat words in tweet series\n",
    "new_lem_tweets = []\n",
    "for item in df['lemmatizer_tweets']:\n",
    "    obj = ''\n",
    "    for w in item:\n",
    "        obj = obj + w + ' '\n",
    "    new_lem_tweets.append(obj)\n",
    "\n",
    "df['lemmatizer_tweets'] = new_lem_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03948607, 0.29783715, 0.33552922, ..., 0.17821177, 0.1847066 ,\n",
       "       0.08015913])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using this as the data target had better performance\n",
    "\n",
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(df['lemmatizer_tweets'])\n",
    "text_tf.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob and VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity = []\n",
    "subjectivity = []\n",
    "for tweet in df['lemmatizer_tweets']:\n",
    "    analysis = TextBlob(tweet)\n",
    "    polar = analysis.sentiment.polarity\n",
    "    sub = analysis.sentiment.subjectivity\n",
    "    polarity.append(polar)\n",
    "    subjectivity.append(sub)\n",
    "    \n",
    "df['textblob_polarity'] = polarity\n",
    "df['textblob_subjectivity'] = subjectivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "vs_neg = []\n",
    "vs_neu = []\n",
    "vs_pos = []\n",
    "vs_compund = []\n",
    "for tweet in df['lemmatizer_tweets']:\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    neg = vs['neg']\n",
    "    neu = vs['neu']\n",
    "    pos = vs['pos']\n",
    "    compound = vs['compound']\n",
    "    \n",
    "    vs_neg.append(neg)\n",
    "    vs_neu.append(neu)\n",
    "    vs_pos.append(pos)\n",
    "    vs_compund.append(compound)\n",
    "\n",
    "    \n",
    "df['vs_neg'] = vs_neg\n",
    "df['vs_neu'] = vs_neu\n",
    "df['vs_pos'] = vs_pos\n",
    "df['vs_compound'] = vs_compund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_files.sentiment_lib as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>product_</th>\n",
       "      <th>emotion</th>\n",
       "      <th>lemmatizer_tweets</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <th>vs_neg</th>\n",
       "      <th>vs_neu</th>\n",
       "      <th>vs_pos</th>\n",
       "      <th>vs_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[wesley83, have, 3G, iPhone, 3, hrs, tweeting,...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>wesley83 have 3G iPhone 3 hr tweeting RISE Aus...</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[jessedee, Know, fludapp, Awesome, iPad, iPhon...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>1</td>\n",
       "      <td>jessedee Know fludapp Awesome iPad iPhone app ...</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.9100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[swonderlin, not, wait, iPad, 2, also, should,...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>1</td>\n",
       "      <td>swonderlin not wait iPad 2 also should sale do...</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[sxsw, hope, year, festival, t, crashy, this, ...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>0</td>\n",
       "      <td>sxsw hope year festival t crashy this year iPh...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.7269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[sxtxstate, great, stuff, Fri, SXSW, Marissa, ...</td>\n",
       "      <td>Google</td>\n",
       "      <td>1</td>\n",
       "      <td>sxtxstate great stuff Fri SXSW Marissa Mayer G...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.6249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>[Ipad, everywhere, SXSW, link]</td>\n",
       "      <td>iPad</td>\n",
       "      <td>1</td>\n",
       "      <td>Ipad everywhere SXSW link</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>[Wave, buzz, RT, mention, interrupt, regularly...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>3</td>\n",
       "      <td>Wave buzz RT mention interrupt regularly sched...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088462</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>[Google, Zeiger, physician, never, reported, p...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>3</td>\n",
       "      <td>Google Zeiger physician never reported potenti...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>[Verizon, iPhone, customers, complained, time,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>3</td>\n",
       "      <td>Verizon iPhone customer complained time fell b...</td>\n",
       "      <td>-0.054545</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>[RT, mention, Google, Tests, Check, Offers, SX...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>3</td>\n",
       "      <td>RT mention Google Tests Check Offers SXSW link</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9092 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet            product_  \\\n",
       "0     [wesley83, have, 3G, iPhone, 3, hrs, tweeting,...              iPhone   \n",
       "1     [jessedee, Know, fludapp, Awesome, iPad, iPhon...  iPad or iPhone App   \n",
       "2     [swonderlin, not, wait, iPad, 2, also, should,...                iPad   \n",
       "3     [sxsw, hope, year, festival, t, crashy, this, ...  iPad or iPhone App   \n",
       "4     [sxtxstate, great, stuff, Fri, SXSW, Marissa, ...              Google   \n",
       "...                                                 ...                 ...   \n",
       "9088                     [Ipad, everywhere, SXSW, link]                iPad   \n",
       "9089  [Wave, buzz, RT, mention, interrupt, regularly...             Unknown   \n",
       "9090  [Google, Zeiger, physician, never, reported, p...             Unknown   \n",
       "9091  [Verizon, iPhone, customers, complained, time,...             Unknown   \n",
       "9092  [RT, mention, Google, Tests, Check, Offers, SX...             Unknown   \n",
       "\n",
       "      emotion                                  lemmatizer_tweets  \\\n",
       "0           0  wesley83 have 3G iPhone 3 hr tweeting RISE Aus...   \n",
       "1           1  jessedee Know fludapp Awesome iPad iPhone app ...   \n",
       "2           1  swonderlin not wait iPad 2 also should sale do...   \n",
       "3           0  sxsw hope year festival t crashy this year iPh...   \n",
       "4           1  sxtxstate great stuff Fri SXSW Marissa Mayer G...   \n",
       "...       ...                                                ...   \n",
       "9088        1                         Ipad everywhere SXSW link    \n",
       "9089        3  Wave buzz RT mention interrupt regularly sched...   \n",
       "9090        3  Google Zeiger physician never reported potenti...   \n",
       "9091        3  Verizon iPhone customer complained time fell b...   \n",
       "9092        3    RT mention Google Tests Check Offers SXSW link    \n",
       "\n",
       "      textblob_polarity  textblob_subjectivity  vs_neg  vs_neu  vs_pos  \\\n",
       "0             -0.200000               0.400000   0.223   0.777   0.000   \n",
       "1              0.466667               0.933333   0.000   0.528   0.472   \n",
       "2             -0.155556               0.288889   0.000   1.000   0.000   \n",
       "3              0.000000               0.000000   0.000   0.596   0.404   \n",
       "4              0.800000               0.750000   0.000   0.796   0.204   \n",
       "...                 ...                    ...     ...     ...     ...   \n",
       "9088           0.000000               0.000000   0.000   1.000   0.000   \n",
       "9089           0.000000               0.088462   0.244   0.756   0.000   \n",
       "9090           0.000000               1.000000   0.000   1.000   0.000   \n",
       "9091          -0.054545               0.218182   0.162   0.838   0.000   \n",
       "9092           0.100000               0.000000   0.000   1.000   0.000   \n",
       "\n",
       "      vs_compound  \n",
       "0         -0.6486  \n",
       "1          0.9100  \n",
       "2          0.0000  \n",
       "3          0.7269  \n",
       "4          0.6249  \n",
       "...           ...  \n",
       "9088       0.0000  \n",
       "9089      -0.4939  \n",
       "9090       0.0000  \n",
       "9091      -0.4019  \n",
       "9092       0.0000  \n",
       "\n",
       "[9092 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.get_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/lemmatized_and_sentiment_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['lemmatizer_tweets']\n",
    "target = df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list with all lemmatized outputs\n",
    "lemmatized_output = []\n",
    "\n",
    "for listy in data:\n",
    "    lemmed = ''.join([w for w in listy])\n",
    "    lemmatized_output.append(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lem = lemmatized_output\n",
    "y_lem = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split the lemmatized words\n",
    "X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X_lem, y_lem, test_size=0.20, random_state=1)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words=stop_words)\n",
    "\n",
    "tfidf_data_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "tfidf_data_test_lem = tfidf.transform(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Number of Non-Zero Elements in Vectorized Tweets\n",
    "non_zero_cols = tfidf_data_train_lem.nnz / float(tfidf_data_train_lem.shape[0])\n",
    "print(non_zero_cols)\n",
    "\n",
    "# Percentage of columns containing ZERO\n",
    "percent_sparse = 1 - (non_zero_cols / float(tfidf_data_train_lem.shape[1]))\n",
    "print(percent_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_lem = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_lem.fit(tfidf_data_train_lem, y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_preds_lem = rf_lem.predict(tfidf_data_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_acc_score_lem = metrics.accuracy_score(y_test_lem, rf_test_preds_lem)\n",
    "rf_f1_score_lem = metrics.f1_score(y_test_lem, rf_test_preds_lem, average='weighted')\n",
    "rf_precision_score_lem = metrics.precision_score(y_test_lem, rf_test_preds_lem, average='weighted')\n",
    "rf_recall_score_lem = metrics.recall_score(y_test_lem, rf_test_preds_lem, average='weighted')\n",
    "print('Accuracy:', rf_acc_score_lem)\n",
    "print('Precision:',rf_precision_score_lem)\n",
    "print('Recall:',rf_recall_score_lem)\n",
    "print('F1:',rf_f1_score_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
